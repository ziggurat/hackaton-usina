{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "1. Crear un .env en la secci√≥n de archivos.\n",
    "2. Subir la carpeta `tramites_db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb langchain-openai langchain-core langchain-chroma langchain-huggingface python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import chromadb\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment Variables\n",
    "Load environment variables and define constants like embedding model ID and prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for embedding model ID and prompt templates\n",
    "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "QUESTION_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Context information is below.\n",
    "    ---------------------\n",
    "    {context}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge, answer the query.\n",
    "    Query: {input}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "TOP_K = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Embeddings and Vector Store\n",
    "Instantiate the embedding model and set up ChromaDB as the vector store with the appropriate collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the embeddings object\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)  # Added embeddings instantiation\n",
    "\n",
    "# Initialize a ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./tramites_db\")\n",
    "\n",
    "# Create a vector store with the embedding function\n",
    "vectorstore = Chroma(\n",
    "    client=chroma_client,\n",
    "    collection_name=\"tramites\",\n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Language Model\n",
    "Create the OpenAI ChatGPT model instance with appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RAG Chain\n",
    "Build the retrieval-augmented generation chain by combining the retriever and question-answering components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(k=TOP_K)\n",
    "\n",
    "# Create the question-answering chain\n",
    "question_answer_chain = create_stuff_documents_chain(model, QUESTION_PROMPT)\n",
    "\n",
    "# Build the retrieval-augmented generation (RAG) chain\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo and Usage\n",
    "Demonstrate how to use the RAG system with example questions and interactive input cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo and Usage\n",
    "\n",
    "# Function to demonstrate the RAG system with example questions\n",
    "def demo_rag_system(rag_chain):\n",
    "    example_questions = [\n",
    "        \"What is the process to renew my passport?\",\n",
    "        \"How can I apply for a driver's license?\",\n",
    "        \"What documents are required for a visa application?\"\n",
    "    ]\n",
    "    \n",
    "    for question in example_questions:\n",
    "        response = rag_chain.invoke({\"input\": question})\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {response['answer']}\\n\")\n",
    "\n",
    "# Run the demo\n",
    "demo_rag_system(rag_chain)\n",
    "\n",
    "# Interactive input cell for user questions\n",
    "def interactive_rag_system(rag_chain):\n",
    "    while True:\n",
    "        question = input(\"Please enter your question (or type 'exit' to quit): \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        response = rag_chain.invoke({\"input\": question})\n",
    "        print(f\"Answer: {response['answer']}\")\n",
    "\n",
    "# Run the interactive system\n",
    "interactive_rag_system(rag_chain)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
